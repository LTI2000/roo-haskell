-- CHANGE WITH CAUTION: This is a generated code file generated by https://github.com/Haskell-OpenAPI-Code-Generator/Haskell-OpenAPI-Client-Code-Generator.

{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE MultiWayIf #-}

-- | Contains the types generated from the schema ChatCompletionRequest
module OpenAI.Generated.Types.ChatCompletionRequest where

import qualified Prelude as GHC.Integer.Type
import qualified Prelude as GHC.Maybe
import qualified Control.Monad.Fail
import qualified Data.Aeson
import qualified Data.Aeson as Data.Aeson.Encoding.Internal
import qualified Data.Aeson as Data.Aeson.Types
import qualified Data.Aeson as Data.Aeson.Types.FromJSON
import qualified Data.Aeson as Data.Aeson.Types.ToJSON
import qualified Data.Aeson as Data.Aeson.Types.Internal
import qualified Data.ByteString
import qualified Data.ByteString as Data.ByteString.Internal
import qualified Data.Foldable
import qualified Data.Functor
import qualified Data.Maybe
import qualified Data.Scientific
import qualified Data.Text
import qualified Data.Text as Data.Text.Internal
import qualified Data.Time.Calendar as Data.Time.Calendar.Days
import qualified Data.Time.LocalTime as Data.Time.LocalTime.Internal.ZonedTime
import qualified GHC.Base
import qualified GHC.Classes
import qualified GHC.Int
import qualified GHC.Show
import qualified GHC.Types
import qualified OpenAI.Generated.Common
import OpenAI.Generated.TypeAlias
import {-# SOURCE #-} OpenAI.Generated.Types.ChatMessage

-- | Defines the object schema located at @components.schemas.ChatCompletionRequest@ in the specification.
-- 
-- 
data ChatCompletionRequest = ChatCompletionRequest {
  -- | max_tokens: Maximum number of tokens to generate
  -- 
  -- Constraints:
  -- 
  -- * Minimum  of 1.0
  chatCompletionRequestMaxTokens :: (GHC.Maybe.Maybe GHC.Types.Int)
  -- | messages: A list of messages comprising the conversation so far
  -- 
  -- Constraints:
  -- 
  -- * Must have a minimum of 1 items
  , chatCompletionRequestMessages :: (GHC.Base.NonEmpty ChatMessage)
  -- | model: ID of the model to use (e.g., \"gpt-3.5-turbo\", \"gpt-4\")
  , chatCompletionRequestModel :: Data.Text.Internal.Text
  -- | stream: Whether to stream partial message deltas
  , chatCompletionRequestStream :: (GHC.Maybe.Maybe GHC.Types.Bool)
  -- | temperature: Sampling temperature between 0 and 2
  -- 
  -- Constraints:
  -- 
  -- * Maxium  of 2.0
  -- * Minimum  of 0.0
  , chatCompletionRequestTemperature :: (GHC.Maybe.Maybe GHC.Types.Double)
  } deriving (GHC.Show.Show
  , GHC.Classes.Eq)
instance Data.Aeson.Types.ToJSON.ToJSON ChatCompletionRequest
    where {toJSON obj = Data.Aeson.Types.Internal.object (Data.Foldable.concat (Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("max_tokens" Data.Aeson.Types.ToJSON..=)) (chatCompletionRequestMaxTokens obj) : ["messages" Data.Aeson.Types.ToJSON..= chatCompletionRequestMessages obj] : ["model" Data.Aeson.Types.ToJSON..= chatCompletionRequestModel obj] : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("stream" Data.Aeson.Types.ToJSON..=)) (chatCompletionRequestStream obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("temperature" Data.Aeson.Types.ToJSON..=)) (chatCompletionRequestTemperature obj) : GHC.Base.mempty));
           toEncoding obj = Data.Aeson.Encoding.Internal.pairs (GHC.Base.mconcat (Data.Foldable.concat (Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("max_tokens" Data.Aeson.Types.ToJSON..=)) (chatCompletionRequestMaxTokens obj) : ["messages" Data.Aeson.Types.ToJSON..= chatCompletionRequestMessages obj] : ["model" Data.Aeson.Types.ToJSON..= chatCompletionRequestModel obj] : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("stream" Data.Aeson.Types.ToJSON..=)) (chatCompletionRequestStream obj) : Data.Maybe.maybe GHC.Base.mempty (GHC.Base.pure GHC.Base.. ("temperature" Data.Aeson.Types.ToJSON..=)) (chatCompletionRequestTemperature obj) : GHC.Base.mempty)))}
instance Data.Aeson.Types.FromJSON.FromJSON ChatCompletionRequest
    where {parseJSON = Data.Aeson.Types.FromJSON.withObject "ChatCompletionRequest" (\obj -> ((((GHC.Base.pure ChatCompletionRequest GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "max_tokens")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "messages")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..: "model")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "stream")) GHC.Base.<*> (obj Data.Aeson.Types.FromJSON..:! "temperature"))}
-- | Create a new 'ChatCompletionRequest' with all required fields.
mkChatCompletionRequest :: GHC.Base.NonEmpty ChatMessage -- ^ 'chatCompletionRequestMessages'
  -> Data.Text.Internal.Text -- ^ 'chatCompletionRequestModel'
  -> ChatCompletionRequest
mkChatCompletionRequest chatCompletionRequestMessages chatCompletionRequestModel = ChatCompletionRequest{chatCompletionRequestMaxTokens = GHC.Maybe.Nothing,
                                                                                                         chatCompletionRequestMessages = chatCompletionRequestMessages,
                                                                                                         chatCompletionRequestModel = chatCompletionRequestModel,
                                                                                                         chatCompletionRequestStream = GHC.Maybe.Nothing,
                                                                                                         chatCompletionRequestTemperature = GHC.Maybe.Nothing}
